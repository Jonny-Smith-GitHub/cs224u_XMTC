{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eurlex_tokenstring.arff') as fp:\n",
    "    data = fp.readlines()[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#doc= 19348\n",
      "avg_doc_len= 1242.8586417200745\n",
      "max_doc_len= 189866\n",
      "min_doc_len= 3\n"
     ]
    }
   ],
   "source": [
    "doc_id = []\n",
    "doc_con = []\n",
    "word_cnt = 0\n",
    "max_len = 0\n",
    "min_len = 1000\n",
    "for line in data:\n",
    "    tup = line.split(',')\n",
    "    words = tup[1][2:-2].split(' ')\n",
    "    doc_id.append(tup[0])\n",
    "    doc_con.append(words)\n",
    "    word_cnt += len(words)\n",
    "    if len(words) > max_len: \n",
    "        max_len = len(words)\n",
    "    elif len(words) < min_len:\n",
    "        min_len = len(words)\n",
    "\n",
    "print('#doc=', len(doc_id))\n",
    "print('avg_doc_len=', word_cnt / len(doc_id))\n",
    "print('max_doc_len=', max_len)\n",
    "print('min_doc_len=', min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id2label = {id: set([]) for id in doc_id}\n",
    "label2id = {}\n",
    "id2label = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file= id2class_eurlex_DC_leaves_l3.qrels\n",
      "file= id2class_eurlex_DC_all.qrels\n",
      "file= id2class_eurlex_subject_matter.qrels\n",
      "file= id2class_eurlex_eurovoc.qrels\n",
      "file= id2class_eurlex_DC_leaves.qrels\n",
      "file= id2class_eurlex_DC_l3.qrels\n",
      "file= id2class_eurlex_DC_l1.qrels\n",
      "file= id2class_eurlex_DC_leaves_l1.qrels\n",
      "file= id2class_eurlex_DC_leaves_l2.qrels\n",
      "file= id2class_eurlex_DC_l2.qrels\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir_name = 'eurlex'\n",
    "os.listdir(dir_name)\n",
    "\n",
    "for file_name in os.listdir(dir_name):\n",
    "    with open(dir_name + '/' + file_name) as fp:\n",
    "        print('file=', file_name)\n",
    "        line = fp.readline()\n",
    "        \n",
    "        while line:\n",
    "            label, docid = line.split(' ')[:2]\n",
    "            if label not in label2id:\n",
    "                label2id[label] = len(label2id)\n",
    "                id2label[label2id[label]] = label\n",
    "            \n",
    "            doc_id2label[docid].add(label2id[label]) \n",
    "            line = fp.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & generate word embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55822, 50)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_emb = []\n",
    "word2id = {}\n",
    "with open('vectors.txt') as fp:\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        line = line[:-1].split(' ')\n",
    "        word_emb.append(np.float_(line[1:]))\n",
    "        word2id[line[0]] = len(word2id)\n",
    "        line = fp.readline()\n",
    "word_emb = np.array(word_emb)\n",
    "word_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate input (word index) & output (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = len(doc_id)\n",
    "inputs = []\n",
    "outputs = np.zeros((len(doc_id), len(label2id)), dtype=np.int32)\n",
    "\n",
    "for i, docid in enumerate(doc_id[:set_size]):\n",
    "    word_list = []\n",
    "    for j, word in enumerate(doc_con[i]):\n",
    "        if word not in word2id: word = '<unk>'\n",
    "        word_list.append(word2id[word])\n",
    "    inputs.append(word_list)\n",
    "    outputs[i, list(doc_id2label[docid])] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sparse input & output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('small_set', x=inputs, y=outputs) # slow\n",
    "np.savez('word_emb', word_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
